---
layout: post
title: 比赛利器：陈天奇的XGBoost
categories: [Machine Learning]
tags: XGBoost 
---

## 背景
2014 年 3 月，XGBOOST 最早作为研究项目，由陈天奇提出。2016年，相关的论文[《XGBoost: A Scalable Tree Boosting System》](https://arxiv.org/pdf/1603.02754.pdf)发表在KDD。XGBoost展开的意思就是Extreme Gradient Boosting，其中Extreme代表极致。工程设计层面的极致包括贪心的排序操作、分割点近似、并发的程序执行；算法层面的极致包括二阶导数、决策树正则项的使用等。本篇博客将会介绍一下几个方面：

- XGBoost与传统GBDT的区别
- 常用的符号表示
- 泰勒公式
- XGBoost中的目标函数（二阶导数）
- XGBoost中的优化方法
- XGBoost中的正则项
- XGBoost中节点的split方法
- XGBoost的并行
- 如何上手XGBoost


## XGBoost与传统GBDT的区别
> 注意：这里所比较的GBDT是以CART作为基分类器的回归树

- 传统GBDT以CART作为基分类器。XGBoost还支持线性分类器（gbtree和gblinear），这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

- 传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了**一阶和二阶导数**。顺便提一下，XGBoost工具支持自定义代价函数，只要函数可一阶和二阶求导。

- XGBoost在代价函数里加入了**正则项**，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost的优点。

- Shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了**削弱每棵树的影响**，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

- 列抽样（column subsampling）。XGBoost借鉴了随机森林RF的做法，支持列抽样（选择部分特征），不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。

- 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。

- XGBoost工具**支持并行**。Boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行**不是tree粒度**的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在**特征粒度**上的。决策树的学习最耗时的一个步骤就是对特征的值进行**预排序**（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为**block结构**，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

- 可并行的**近似直方图算法**。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了可并行的近似直方图算法，用于高效地生成候选的分割点。


## 常用的符号表示

| 符号 | 含义 |
|:-----------:|:--------------------:|
| $$d$$           | 特征数个数  |
| $$n$$           | 样本个数  |
| $$R^d$$           | 特征数为$$d$$的数据集  |
| $$x_i \in R^d$$   | 第$$i$$个样本         |
| $$w_j$$           | 第$$j$$个特征的权重    |
| $$\widehat{y_i}$$ | $$x_i$$的预测值        |
| $$\Theta$$        | 特征权重的集合, $$\Theta={\{w_j\|j=1, \cdots ,d\}}$$ |
| $$T$$             | 叶子节点个数           |


## 泰勒公式

泰勒公式是一个用函数在某点的信息描述其附近取值的公式，可以理解为**局部有效性**。其基本形式如下：

$$
\begin{equation}
   f(x) = \sum\limits_{n=0}^{\infty} \frac{f^{(n)} (x_0)}{n!} {(x-x_0)}^{n}
\end{equation}
$$

- 一阶泰勒展开： $f(x) \approx f(x_0) + f^{\prime} (x_0)(x-x_0)$
- 二阶泰勒展开： $f(x) \approx f(x_0) + f^{\prime} (x_0)(x-x_0) + f^{\prime\prime}(x_0) \frac{(x-x_0)^2}{2}$

假设$x^t = x^{t-1} + \Delta x$，将$f(x^t)$在$x^{(t-1)}$处进行泰勒展开，其迭代形式如下：

$$
\begin{aligned}
   f(x^t) & = f(x^{t-1} + \Delta{x}) \\
          & \approx f(x^{t-1}) + f^{\prime}(x^{t-1}) \Delta{x} + f^{\prime\prime}(x^{t-1}) \frac{(\Delta x)^2}{2}
\end{aligned}
$$


## XGBoost中的目标函数

### Boosting加法模型
假设我们有K棵树，根据Boosting的加法模型，那么：

$$
\begin{equation}
   \widehat{y_i} = \sum\limits_{k=1}^{K} f_k(x_i), f_k \in \boldsymbol{F}
\end{equation}
$$

上式中$\boldsymbol{F}$表示的是回归森林中的所有函数空间；$f_k$表示一棵树，包括树的结构和叶子节点权重；$f_k(x_i)$表示的是第$i$个样本在第$k$棵树中落在自叶子的权重。以下图为例：

![example-tree-ensemble](/assets/images/blog/xgboost/example-tree-ensemble.png)

可见，男孩落在第一棵树的最左叶子和第二棵树的最左叶子，所以他的得分就是两个叶子节点的权重之和。

那么，我们需要求的参数就是**每棵树的结构**和**叶子节点权重**，简单来看就是$f_k$。用前面的**符号表示**进行结构统一，可以有：

$$
\begin{equation}
   \Theta= \{f_1, f_2, f_3 \cdots f_k\}
\end{equation}
$$

注意，这里$\Theta$理解为所求参数的集合，里面的$f$和上文中的$w$本质上是一致的。
这些参数都是**未知待求解**的，即下文目标函数和优化方法要求解的。

### 目标函数
$$
\begin{aligned}
    Obj = \begin{matrix} \underbrace{ \sum\limits_{i=1}^{n} L(y_i, \widehat{y_i}) } \\ {Training\ loss} \end{matrix} + 
          \begin{matrix} \underbrace{ \sum\limits_{k=1}^{K} \Omega(f_k) } \\ {Regularization} \end{matrix}
\end{aligned}
$$

- 第一项是针对 $n$ 个样本的 $Loss$, 它可以有多种选择
    - 绝对值误差，$L(y_i, \widehat{y_i}) = \|(y_i - \widehat{y_i})\|$
    - 平方误差，$L(y_i, \widehat{y_i}) = {(y_i - \widehat{y_i})}^2$，此时叫做gradient boosted machine
    - logistic loss，$L(y_i, \widehat{y_i}) = y_i \ln(1 + \exp(-\widehat{y_i}) + (1-y_i)\ln(1+\exp(\widehat{y_i})))$，此时叫做logistBoost
- 第二项是正则项，即树的复杂度，下文重点分析。


## XGBoost中的Regularization

在XGBoost的目标函数中，$\Omega(f_t)$是代表模型的复杂度。这个部分在GBDT最初提出的时候并不完善，1999年Friedman的论文[《GREEDY FUNCTION APPROXIMATION:A GRADIENT BOOSTING MACHINE》](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451)中更多的是依靠**学习率**和**Boosting树的规模**两个因素来进行模型选择。

![first-regularization-gbdt](/assets/images/blog/xgboost/first-regularization-gbdt.png)

2014年，Rie Johnson和Tong Zhang在[《Learning Nonlinear Functions Using Regularized Greedy Forest》](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6583153)针对目标函数中正则项的部分提出了较为全面的分析。后续，这些正则思想在XGBoost中也得到了进一步的应用，主要包括：
- 树的深度
- 最小叶子权重
- 叶子个数
- 叶子权重的平滑程度

这里，用叶子个数和叶子权重的平滑程度来形式化描述模型的复杂度，可以得到：

$$
\begin{equation}
   \Omega(f_t) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}
\end{equation}
$$

上式中，第一项利用叶子个数$T$乘以一个收缩系数$\gamma$，第二项用L2范数来表示叶子权重的平滑程度。下图就是计算复杂度的一个示例。

![example-regularization-xgboost](/assets/images/blog/xgboost/example-regularization-xgboost.png)


## XGBoost中的Training Loss

首先，我们先细化上述目标函数中training loss的生成过程，其想法很简单，一棵树一棵树往上加，一直到K棵树停止。过程可以如下：

$$
\begin{cases}
    \widehat{y_i}^{(0)} = 0\\
    \widehat{y_i}^{(1)} = f_1(x_i) = \widehat{y_i}^{(0)} + f_1(x_i)\\
    \widehat{y_i}^{(2)} = f_1(x_i) + f_2(x_i) = \widehat{y_i}^{(1)} + f_2(x_i)\\
    \cdots \\
    \widehat{y_i}^{(t)} = \sum\limits_{k=1}^t f_k(x_i) = \widehat{y_i}^{(t-1)} + f_t(x_i)\\
\end{cases}
$$

其中，$\widehat{y_i}^{(t)}$表示第$t$次迭代后，样本$x_i$所得到的得分。


## XGBoost中的优化方法

将*Training Loss*和*Regularization*带入目标函数，可得：

$$
\begin{aligned}
    Obj^{(t)} & = \sum\limits_{i=1}^{n} L(y_i, \widehat{y_i}^{(t)}) + \sum\limits_{i=1}^{t} \Omega(f_i) \\
              & = \sum\limits_{i=1}^{n} L \left (y_i, \widehat{y_i}^{(t-1)} + f_t(x_i) \right ) + \Omega(f_t) + constant \\
\end{aligned}
$$

然后，应用**泰勒公式**，将$\widehat{y_i}^{(t)}$看作$f(x + \Delta{x})$，$\widehat{y_i}^{(t-1)}$对应$f(x)$，$f_t(x_i)$对应$\Delta{x}$。

$$
\begin{aligned}
    f(x+\Delta x) \approx f(x) + f^{\prime}(x) \Delta{x} + \frac{1}{2} f^{\prime\prime}(x)(\Delta x)^2
\end{aligned}
$$

为了方便推导，这里做一些简单设定：
$$
\begin{cases}
    g_i = f^{(\prime)}(x) = \frac{\partial L(y_i, \  \widehat{y_i}^{(t-1)})}{\partial \widehat{y_i}^{(t-1)}} \\
    \ \\
    h_i = f^{(\prime\prime)}(x) = \frac{\partial^{2} L(y_i, \  \widehat{y_i}^{(t-1)})}{\partial({\widehat{y_i}^{(t-1)}})^{2}}
\end{cases}
$$

根据**泰勒公式**和 $g_i$ 和 $h_i$ 的设定，最终得到下式：

$$
\begin{aligned}
    Obj^{(t)} 
    & = \sum\limits_{i=1}^{n} L \left ( y_i, \widehat{y_i}^{(t-1)} + f_t(x_i) \right ) + \Omega(f_t) + constant \\
    & \approx \sum\limits_{i=1}^{n} 
        \left [ L(y_i, \widehat{y_i}^{(t-1)}) + 
        \begin{matrix} 
            \underbrace{ \frac{\partial L(y_i, \  \widehat{y_i}^{(t-1)})}{\partial \widehat{y_i}^{(t-1)}} } \\ {g_i}
        \end{matrix}
        \begin{matrix} \underbrace{ f_t(x_i) } \\ {\Delta{x}} \end{matrix} + 
        \frac{1}{2} 
        \begin{matrix} 
            \underbrace{ \frac{\partial^{2} L(y_i, \  \widehat{y_i}^{(t-1)})}{\partial({\widehat{y_i}^{(t-1)}})^{2}} } \\ {h_i} 
        \end{matrix} 
        \begin{matrix} \underbrace{ f_t^2(x_i) } \\ {(\Delta{x})^2} \end{matrix}
        \right ] + \Omega(f_t) + constant \\
    & = \sum\limits_{i=1}^{n} \left [L(y_i, \widehat{y_i}^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \Omega(f_t) + constant \\
    & = \sum\limits_{i=1}^{n} \left [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \Omega(f_t) + \left [\sum\limits_{i=1}^{n} L(y_i, \widehat{y_i}^{(t-1)}) + constant \right ]
\end{aligned}
$$

很容易看出，上述公式中的第二部分 $\left [\sum\limits_{i=1}^{n} L(y_i, \widehat{y_i}^{(t-1)}) + constant \right ]$ 对于目标函数最优值的求解**无任何影响**（因为里面没有$f_t(x_i)$的相关信息），所以，现在把优化函数写作下面的形式：

$$
\begin{aligned}
    Obj^{(t)} \approx \sum\limits_{i=1}^{n} \left [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \Omega(f_t)
\end{aligned}
$$

我们已经知道，$f_t(x)$的物理意义（前文中的$f_k(x)$），它就是一棵树，重点是需要求解出它的参数，即**树的结构**和**叶子节点权重**，我们现在进一步形式化这棵树。设$w \in R^(T)$，$w$ 为**树叶权重序列**，$T$ 为叶子节点个数。$q\ :R^{(T)} \to \{1,2, \cdots, T\}$, $q$为**树结构**。那么$q(x)$表示的就是样本 $x$ 所落在树叶的位置。这里可以用下图简单的理解：

![example-tree-structure](/assets/images/blog/xgboost/example-tree-structure.png)

于是，$f_t(x)$可以用下式进行表示：
$$
\begin{aligned}
    f_t(x) = w_{q(x)}, \ w \in R^T, \ q\ :R^{(T)} \to \{1,2, \cdots, T\}
\end{aligned}
$$

最后，我们在增加一个定义，用 $I_j$ 来表示第j个叶子里的样本集合。也就是上图中，第 $j$ 个圈，就用 $I_j$ 来表示。

$$
\begin{aligned}
    I_j = \{i|\ q(x_i) = j\}
\end{aligned}
$$

所以到目前位置，我们要求解的树模型就是关于这两个变量$I_j$和$w_j$（理解为，第 $j$ 棵树的结构和叶子节点权重），最终的优化函数可以**针对这两个变量**展开（舍弃常数项）：

$$
\begin{aligned}
    Obj^{(t)} 
    & = \sum\limits_{i=1}^{n} \left [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \Omega(f_t) + \left [\sum\limits_{i=1}^{n} L(y_i, \widehat{y_i}^{(t-1)}) + constant \right ] \\
    & \approx \sum\limits_{i=1}^{n} \left [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \Omega(f_t)\\
    & = \sum\limits_{i=1}^{n} \left [g_i w_{q(x_i)} + \frac{1}{2} h_i {w_{q(x_i)}}^2 \right ] + \gamma T + \frac{1}{2}    \lambda \sum_{j=1}^{T} w_{j}^{2}\\
    & = \sum\limits_{j=1}^T \left [(
        \begin{matrix} 
            \underbrace{ \sum_{i \in I_j} g_i } \\ {G_j} 
        \end{matrix}) w_j + 
        \frac{1}{2}(
        \begin{matrix} 
            \underbrace{ \sum_{i \in I_j} h_i } \\ {H_j} 
        \end{matrix} + \lambda) w_j^2 \right ] + \gamma T \\
    & = \sum\limits_{j=1}^T \left (G_j w_j + \frac{1}{2} (H_i + \lambda) {w_j}^2 \right ) + \gamma T
\end{aligned}
$$


## XGBoost中节点的split方法

## XGBoost的并行

## 如何上手XGBoost


---
# 相关引用
1. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)
2. [Learning Nonlinear Functions Using Regularized Greedy Forest](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6583153)
3. [XGBoost论文阅读及其原理](https://zhuanlan.zhihu.com/p/36794802)
4. [GBDT与XGBoost](http://sofasofa.io/forum_main_post.php?postid=1000331)
5. [GBDT by wepon](http://wepon.me/files/gbdt.pdf)