---
layout: post
title: 比赛利器：LightGBM
categories: [Machine Learning]
tags: LightGBM 
---

## 背景

微软 DMTK (分布式机器学习工具包)团队在 GitHub 上开源了性能超越其他 boosting 工具的 LightGBM 知乎上有近千人关注“如何看待微软开源的LightGBM？”问题，被评价为“速度惊人”，“非常有启发”，“支持分布式”，“代码清晰易懂”，“占用内存小”等。

## GBDT相关知识

GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT 在工业界应用广泛，通常被用于点击率预测，搜索排序等任务。GBDT 也是各种数据挖掘竞赛的致命武器，据统计 Kaggle 上的比赛有一半以上的冠军方案都是基于 GBDT。 

## LightGBM相关实验

[LightGBM （Light Gradient Boosting Machine）](https://github.com/Microsoft/LightGBM)是一个实现 GBDT 算法的框架，支持高效率的并行训练，并且具有以下优点([中文参考](http://lightgbm.apachecn.org/cn/latest/Quick-Start.html))： 
- 更快的训练速度 
- 更低的内存消耗 
- 更好的准确率 
- 分布式支持，可以快速处理海量数据

从 LightGBM 的 GitHub 主页上可以直接看到实验结果

1. 从下图实验数据可以看出，在 Higgs 数据集上 LightGBM 比 XGBoost 快将近 10 倍，内存占用率大约为 XGBoost 的1/6，并且准确率也有提升。在其他数据集上也可以观察到相似的结论。 

![exp-1](http://yongyuan.name/imgs/posts/decision_tree_1.png)

再举个具体的例子：

![drawing](http://yongyuan.name/imgs/posts/decision_tree_2.png)


## LightGBM里的Trick

### 1. Histogram 算法

### 2. 带深度限制的 Leaf-wise 的叶子生长策略

### 3. 直方图差加速

### 4. 直接支持类别特征

### 5. 小结
---

## LightGBM 高效并行
在探寻了 LightGBM 的优化之后，发现 LightGBM 还具有支持高效并行的优点。LightGBM 原生支持并行学习，目前支持特征并行和数据并行的两种。特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。LightGBM 针对这两种并行方法都做了优化，在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信；在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。更具体的内容可以看我们在 [NIPS2016 的文章](https://arxiv.org/abs/1611.01276)。 


---
# 相关引用
1. [开源LightGBM基本原理及调用形式](https://cloud.tencent.com/developer/article/1141171)