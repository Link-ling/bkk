---
layout: post
title: GBDT家族
categories: [Machine Learning]
tags: GBDT 
---

## 背景

1999年，Friedman的论文[《GREEDY FUNCTION APPROXIMATION:A GRADIENT BOOSTING MACHINE》](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451)最早提出GBDT (Gradient Boosting Decision Tree) 的思想。到目前为止，GBDT是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。GBDT 在工业界应用广泛，通常被用于点击率预测，搜索排序等任务。GBDT 也是各种数据挖掘竞赛的致命武器，据统计 Kaggle 上的比赛有一半以上的冠军方案都是基于 GBDT。 

其模型 $F$ 定义为**加法模型**：

$$
\begin{equation}
   F(x;w) = \sum_{t=0}^{T} \alpha_t h_t(x;w_t) = \sum_{t=0}^{T} f_t(x;w_t)
\end{equation}
$$

- 其中，$x$为输入样本，$h$为分类回归树（CART），$w$是分类回归树的参数，$\alpha$是每棵树的权重。
- 注意，在[Friedman, 1999](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451)文章中，没有涉及正则项$\Omega(f_t)$这个概念。正则项的真正使用在[Rie Johnson & Tong Zhang, 2014](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6583153)和[Chen, 2016](https://arxiv.org/pdf/1603.02754.pdf)，两者大同小异，但正则项$\Omega(f_t)$的使用对于GBDT来讲有里程碑意义，它对提升训练效果非常重要。


## Boosting思想

不同的objective和最小化其的方法决定了不同种类的Boosting:

![boosting-objective](/assets/images/blog/gbdt/boosting-objective.jpg)
GBDT是上图中的Gradient Boosting的一个子类（弱分类器为决策树）


## GBDT基础模型

基本上所有的机器学习模型都是下面公式演化而来

$$
\begin{equation}
   \widehat{y_i} = \sum_{i=1}^{d} w_j x_{ij}
\end{equation}
$$

上述公式中$x_0=1$，就是引入了一个偏差量，或者说加入了一个常数项。由该公式可以演化一些模型：
- 线性模型，最后的得分就是$\widehat{y_i}$
- logistic模型，最后的得分是$\frac{1}{exp(-\widehat{y_i})}$，然后设定阈值，转为正负样本
- 其他的大部分模型，都在基于$\widehat{y_i}$做文章，转化为最后的分数


---
# 相关引用
1. [Friedman: GREEDY FUNCTION APPROXIMATION:A GRADIENT BOOSTING MACHINE](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451)
2. [Learning Nonlinear Functions Using Regularized Greedy Forest](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6583153)
3. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)
4. [GBDT的小结](https://blog.csdn.net/niuniuyuh/article/details/76922210)
5. [GBDT简单理解](https://blog.csdn.net/meanme/article/details/50914222)