---
layout: post
title: 由浅入深搞定最大熵模型
categories: [Machine Learning]
tags: MaximumEntropyModel
---

### Index
<!-- TOC -->
- [背景](#背景)
- [关于熵的几个概念](#关于熵的几个概念)
- [最大熵原理](#最大熵原理)
- [最大熵模型](#最大熵模型)
<!-- /TOC -->

---
## 背景
有这么几个概念，你懂不？

- 自信息(Self-information)
- 熵(Entropy)
- 联合熵(Joint Entropy)
- 条件熵(Conditional Entropy)
- 相对熵(Relative Entropy，也称为KL散度，Kullback–Leibler divergence)
- 交叉熵(Cross entropy)
- 互信息(Mutual-information)

懂了你就跳过去，不懂咱就先讲讲，最大熵原理和模型最后再说。


## 关于熵的几个概念
话不多说，我先上个图，理解了这个图，下面的介绍就都没问题了。
<!-- ![1](/assets/images/blog/maximumEntropyModel/1.png) -->
![2](/assets/images/blog/maximumEntropyModel/2.png)
<img src="/assets/images/blog/maximumEntropyModel/2.png" width="50" hegiht="50" align=center />



### 自信息(Self-information)
### 熵(Entropy)
### 联合熵(Joint Entropy)
### 条件熵(Conditional Entropy)
### 相对熵(Relative Entropy，也称为KL散度，Kullback–Leibler divergence)
### 交叉熵(Cross entropy)
### 互信息(Mutual-information)


## 最大熵原理


## 最大熵模型

## 极大似然 等价于 最大熵模型

---
# 相关引用
1. [详解机器学习中的熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/kyrieng/p/8694705.html)
2. [一步一步理解最大熵模型](https://www.cnblogs.com/wxquare/p/5858008.html)
3. [相对熵-百度百科](https://baike.baidu.com/item/%E7%9B%B8%E5%AF%B9%E7%86%B5/4233536?fr=aladdin)
4. [信息论：熵与互信息](https://blog.csdn.net/pipisorry/article/details/51695283)
5. [最大熵模型中的对数似然函数的解释](https://blog.csdn.net/wkebj/article/details/77965714)
6. [概率论：p(x\|theta)和p(x;theta)的区别](https://blog.csdn.net/pipisorry/article/details/42715245)
7. [一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉](https://blog.csdn.net/tsyccnh/article/details/79163834)
<!-- \|表示转义 -->